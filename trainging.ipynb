{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy opencv-python scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23be11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2762 - loss: 2.8058 - val_accuracy: 0.6654 - val_loss: 2.2484\n",
      "Epoch 2/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7586 - loss: 1.6026 - val_accuracy: 0.8574 - val_loss: 1.0072\n",
      "Epoch 3/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9114 - loss: 0.6616 - val_accuracy: 0.9316 - val_loss: 0.4737\n",
      "Epoch 4/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9452 - loss: 0.3262 - val_accuracy: 0.9525 - val_loss: 0.3131\n",
      "Epoch 5/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.2035 - val_accuracy: 0.9639 - val_loss: 0.2268\n",
      "Epoch 6/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.1384 - val_accuracy: 0.9715 - val_loss: 0.1773\n",
      "Epoch 7/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9895 - loss: 0.0998 - val_accuracy: 0.9810 - val_loss: 0.1458\n",
      "Epoch 8/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0732 - val_accuracy: 0.9829 - val_loss: 0.1222\n",
      "Epoch 9/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9962 - loss: 0.0550 - val_accuracy: 0.9829 - val_loss: 0.1067\n",
      "Epoch 10/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0429 - val_accuracy: 0.9848 - val_loss: 0.0944\n",
      "Epoch 11/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0340 - val_accuracy: 0.9829 - val_loss: 0.0859\n",
      "Epoch 12/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0274 - val_accuracy: 0.9829 - val_loss: 0.0774\n",
      "Epoch 13/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9981 - loss: 0.0224 - val_accuracy: 0.9829 - val_loss: 0.0715\n",
      "Epoch 14/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0186 - val_accuracy: 0.9829 - val_loss: 0.0667\n",
      "Epoch 15/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0155 - val_accuracy: 0.9848 - val_loss: 0.0639\n",
      "Epoch 16/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0134 - val_accuracy: 0.9829 - val_loss: 0.0615\n",
      "Epoch 17/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0116 - val_accuracy: 0.9829 - val_loss: 0.0575\n",
      "Epoch 18/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0100 - val_accuracy: 0.9829 - val_loss: 0.0569\n",
      "Epoch 19/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0088 - val_accuracy: 0.9829 - val_loss: 0.0534\n",
      "Epoch 20/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0078 - val_accuracy: 0.9829 - val_loss: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. CARGAR CSV\n",
    "# -------------------------------------------------------\n",
    "df = pd.read_csv(\"dataset_with_noise.csv\")\n",
    "\n",
    "# Ruta de la imagen\n",
    "paths = df[\"image_path\"].values\n",
    "labels = df[\"label\"].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. CARGAR Y NORMALIZAR IMÁGENES\n",
    "# -------------------------------------------------------\n",
    "IM_SIZE = 64  # tamaño estándar para reducir dimensionalidad\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (IM_SIZE, IM_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    return img.flatten()  # vector\n",
    "\n",
    "X = np.array([load_image(p) for p in paths])\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. ENCODE LABELS\n",
    "# -------------------------------------------------------\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. PCA PARA REDUCIR VARIABLES ****  \n",
    "# -------------------------------------------------------\n",
    "pca = PCA(n_components=100)   # reduce la imagen a 100 features\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. TRAIN / TEST SPLIT\n",
    "# -------------------------------------------------------\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X_pca, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. RED NEURONAL SIMPLE\n",
    "# -------------------------------------------------------\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(100,)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(y.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. ENTRENAMIENTO\n",
    "# -------------------------------------------------------\n",
    "model.fit(\n",
    "    Xtrain, ytrain,\n",
    "    validation_data=(Xtest, ytest),\n",
    "    batch_size=32,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8. GUARDAR MODELO Y PCA\n",
    "# -------------------------------------------------------\n",
    "model.save(\"sign_model.h5\")\n",
    "np.save(\"pca_components.npy\", pca.components_)\n",
    "np.save(\"pca_mean.npy\", pca.mean_)\n",
    "np.save(\"label_classes.npy\", le.classes_)\n",
    "print(\"Modelo guardado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07000af",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd5212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA dimensiones reducidas: 9\n",
      "Accuracy en validación: 0.9961977186311787\n",
      "Modelo Random Forest guardado como rf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# CARGA DEL CSV\n",
    "# -------------------------------------------------------------\n",
    "df = pd.read_csv(\"dataset_with_noise.csv\")\n",
    "\n",
    "# Etiquetas variable objetivo\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Guardar las clases\n",
    "label_classes = np.unique(y)\n",
    "np.save(\"rf_label_classes.npy\", label_classes)\n",
    "\n",
    "# Convertir letras a índices\n",
    "label_to_idx = {c: i for i, c in enumerate(label_classes)}\n",
    "y_idx = y.map(label_to_idx)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# FEATURES NUMÉRICAS (TODO EXCEPTO PATH Y LABEL)\n",
    "# -------------------------------------------------------------\n",
    "feature_cols = [\n",
    "    \"x\",\"y\",\"z\",\n",
    "    \"thumb\",\"fore\",\"index\",\"ring\",\"little\",\n",
    "    \"thumb2\",\"fore2\",\"index2\",\"ring2\",\"little2\",\n",
    "    \"keycode\",\"gs1\",\"gs2\",\"sign\",\n",
    "    \"giroX\",\"giroY\",\"giroZ\"\n",
    "]\n",
    "\n",
    "X = df[feature_cols].values.astype(\"float32\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# NORMALIZACIÓN\n",
    "# -------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Guardar scaler\n",
    "joblib.dump(scaler, \"rf_scaler.pkl\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# PCA\n",
    "# -------------------------------------------------------------\n",
    "pca = PCA(n_components=0.95, svd_solver='full')  # conserva 95% varianza\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "np.save(\"rf_pca_components.npy\", pca.components_)\n",
    "np.save(\"rf_pca_mean.npy\", pca.mean_)\n",
    "\n",
    "print(\"PCA dimensiones reducidas:\", X_pca.shape[1])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# SPLIT\n",
    "# -------------------------------------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_pca, y_idx, test_size=0.2, random_state=42, stratify=y_idx\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# RANDOM FOREST\n",
    "# -------------------------------------------------------------\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# EVALUACIÓN\n",
    "# -------------------------------------------------------------\n",
    "acc = rf.score(X_val, y_val)\n",
    "print(\"Accuracy en validación:\", acc)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# GUARDAR MODELO\n",
    "# -------------------------------------------------------------\n",
    "joblib.dump(rf, \"rf_model.pkl\")\n",
    "\n",
    "print(\"Modelo Random Forest guardado como rf_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3472e7e",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a60ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MÉTRICAS DEL MODELO SVM =====\n",
      "Accuracy:  0.9954\n",
      "Precision: 0.9956\n",
      "Recall:    0.9954\n",
      "F1-Score:  0.9954\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        25\n",
      "           1       1.00      0.96      0.98        25\n",
      "           2       1.00      0.96      0.98        25\n",
      "           3       0.96      1.00      0.98        25\n",
      "           4       1.00      1.00      1.00        25\n",
      "           5       1.00      1.00      1.00        25\n",
      "           6       1.00      1.00      1.00        26\n",
      "           7       1.00      1.00      1.00        26\n",
      "           8       1.00      1.00      1.00        25\n",
      "           9       1.00      1.00      1.00        25\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      1.00      1.00        25\n",
      "          12       1.00      1.00      1.00        25\n",
      "          13       1.00      1.00      1.00        25\n",
      "          14       1.00      1.00      1.00        26\n",
      "          15       1.00      1.00      1.00        25\n",
      "          16       1.00      1.00      1.00        25\n",
      "          17       1.00      1.00      1.00        25\n",
      "          18       1.00      1.00      1.00        25\n",
      "          19       1.00      1.00      1.00        26\n",
      "          20       1.00      1.00      1.00        51\n",
      "          21       1.00      1.00      1.00        25\n",
      "          22       1.00      1.00      1.00        25\n",
      "          23       1.00      0.96      0.98        26\n",
      "          24       0.96      1.00      0.98        25\n",
      "\n",
      "    accuracy                           1.00       657\n",
      "   macro avg       1.00      1.00      1.00       657\n",
      "weighted avg       1.00      1.00      1.00       657\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0 24  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 1  0 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0 25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0 25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0 25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0 26  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0 26  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 26  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 26  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25\n",
      "   1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  25]]\n",
      "\n",
      "✔ Modelo SVM entrenado, evaluado y guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_svm(csv_path=\"dataset.csv\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 1. Cargar y procesar imágenes\n",
    "    # -------------------------------------\n",
    "    images = []\n",
    "    for path in df[\"image_path\"]:\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "        img = img.flatten() / 255.0\n",
    "        images.append(img)\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 2. Features numéricos del CSV\n",
    "    # -------------------------------------\n",
    "    numeric_cols = [\n",
    "        \"x\",\"y\",\"z\",\n",
    "        \"thumb\",\"fore\",\"index\",\"ring\",\"little\",\n",
    "        \"thumb2\",\"fore2\",\"index2\",\"ring2\",\"little2\",\n",
    "        \"gs1\",\"gs2\",\n",
    "        \"giroX\",\"giroY\",\"giroZ\"\n",
    "    ]\n",
    "\n",
    "    csv_features = df[numeric_cols].values.astype(float)\n",
    "\n",
    "    # unir imagen + csv\n",
    "    X = np.hstack([images, csv_features])\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 3. Labels\n",
    "    # -------------------------------------\n",
    "    labels = df[\"label\"]\n",
    "    label_classes = np.unique(labels)\n",
    "    label_to_idx = {c: i for i, c in enumerate(label_classes)}\n",
    "    y = np.array([label_to_idx[l] for l in labels])\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 4. Train Test Split\n",
    "    # -------------------------------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 5. Normalizar\n",
    "    # -------------------------------------\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 6. PCA\n",
    "    # -------------------------------------\n",
    "    pca = PCA(0.97)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 7. Entrenar SVM\n",
    "    # -------------------------------------\n",
    "    svm = SVC(kernel=\"rbf\", probability=True)\n",
    "    svm.fit(X_train_pca, y_train)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 8. Evaluar\n",
    "    # -------------------------------------\n",
    "    y_pred = svm.predict(X_test_pca)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== MÉTRICAS DEL MODELO SVM =====\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(\"\\nReporte de Clasificación:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    print(\"Matriz de Confusión:\")\n",
    "    print(cm)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 9. Guardar modelos\n",
    "    # -------------------------------------\n",
    "    joblib.dump(svm, \"svm_model.pkl\")\n",
    "    joblib.dump(scaler, \"svm_scaler.pkl\")\n",
    "    joblib.dump(pca, \"svm_pca.pkl\")\n",
    "    np.save(\"svm_label_classes.npy\", label_classes)\n",
    "\n",
    "    print(\"\\n✔ Modelo SVM entrenado, evaluado y guardado correctamente.\")\n",
    "\n",
    "train_svm(\"dataset_with_noise.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
